{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ixpt25YynGE7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Adjust the file path according to your Google Colab environment\n",
        "file_path = '/content/BenchmarkUddinSO-ConsoliatedAspectSentiment.xls'\n",
        "data = pd.read_excel(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace 'sent', 'ManualLabel', and 'code' with the actual column names from your dataset\n",
        "df = data[['sent', 'ManualLabel', 'codes']]\n",
        "\n",
        "# Filter for the 'Usability' aspect; adjust the 'code' column name as necessary\n",
        "df_usability = df[df['codes'].str.contains('Usability', case=False)]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df_usability, test_size=0.2, random_state=42)\n",
        "\n",
        "# Prepare the labels for binary classification ('p' for positive as 1, 'n' for negative as 0)\n",
        "train_labels = train['ManualLabel'].apply(lambda x: 1 if x == 'p' else 0).tolist()\n",
        "test_labels = test['ManualLabel'].apply(lambda x: 1 if x == 'p' else 0).tolist()\n"
      ],
      "metadata": {
        "id": "DDM13KhMs9bT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fePuny9tcVw",
        "outputId": "f9dd94ab-9430-462a-dc80-97b236e012c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "QgwAcTc6u7Dt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Tokenize the text\n",
        "train_encodings = tokenizer(train['sent'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test['sent'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# Define the dataset class\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train_labels)\n",
        "test_dataset = SentimentDataset(test_encodings, test_labels)\n"
      ],
      "metadata": {
        "id": "YkaNJbzps9vO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3foK-EOvT8g",
        "outputId": "afda4f9e-eb17-4833-d8ee-d58caf3f41e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/290.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/290.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.99)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.28.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install transformers[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQdNbKA-vqqs",
        "outputId": "2cfe64cd-102c-4ded-fa89-ed62f58ed558"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "\u001b[1;31mE: \u001b[0mUnable to locate package transformers[torch]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8SFsZngwgnz",
        "outputId": "6a800549-0ba4-4da3-f052-e8002a6a9bf2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset\n",
        "data = pd.read_excel('/content/BenchmarkUddinSO-ConsoliatedAspectSentiment.xls')\n",
        "\n",
        "# Assuming the dataset has columns 'sent', 'ManualLabel', and 'code' for review, label, and aspect\n",
        "df_usability = data[data['codes'].str.contains('Usability', case=False)]\n",
        "\n",
        "train, test = train_test_split(df_usability, test_size=0.2, random_state=42)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "train_encodings = tokenizer(train['sent'].tolist(), truncation=True, padding=True, max_length=512)\n",
        "test_encodings = tokenizer(test['sent'].tolist(), truncation=True, padding=True, max_length=512)\n"
      ],
      "metadata": {
        "id": "M1EwHgxiwjV3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = SentimentDataset(train_encodings, train['ManualLabel'].apply(lambda x: 1 if x == 'p' else 0).tolist())\n",
        "test_dataset = SentimentDataset(test_encodings, test['ManualLabel'].apply(lambda x: 1 if x == 'p' else 0).tolist())\n"
      ],
      "metadata": {
        "id": "EghcfBt9wq1m"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2)\n",
        "model.train()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):  # Number of epochs\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        optim.zero_grad()\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch}: Loss {total_loss / len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Q40hXUwtte",
        "outputId": "20d2a59e-b06c-4a4c-e99a-a9e354a3cfad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 0.6267516045934625\n",
            "Epoch 1: Loss 0.6299960729148653\n",
            "Epoch 2: Loss 0.6264564225243198\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEPhlhVyYm80",
        "outputId": "2aabc878-6e79-4937-b4aa-a4a85556d16c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "V9SSvmmVYpo_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "model.eval()\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "start_time = time.time()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        attention_mask = batch['attention_mask']\n",
        "        labels = batch['labels']\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "        predictions.append(logits)\n",
        "        true_labels.append(label_ids)\n",
        "\n",
        "inference_time = time.time() - start_time\n",
        "print(f\"Inference time: {inference_time} seconds\")\n",
        "\n",
        "predictions = [item for sublist in predictions for item in sublist]\n",
        "true_labels = [item for sublist in true_labels for item in sublist]\n",
        "predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predictions)\n",
        "f1 = f1_score(true_labels, predictions)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1 Score: {f1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN7FpelfaL46",
        "outputId": "12ae9255-ddb4-4b4a-c5f5-2a5083556776"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference time: 316.178484916687 seconds\n",
            "Accuracy: 0.6631944444444444\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'ManualLabel' is your label column\n",
        "class_counts = train['ManualLabel'].value_counts()\n",
        "print(class_counts)\n",
        "class_counts.plot(kind='bar')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Class Distribution in Training Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "ZgIAE2u5Yuay",
        "outputId": "9c523eeb-1ea8-45b8-9e4a-85f2705dc31d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "o    541\n",
            "p    363\n",
            "n    245\n",
            "Name: ManualLabel, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Get unique labels\n",
        "unique_labels = train['ManualLabel'].unique()\n",
        "\n",
        "# Map labels to a binary format if necessary\n",
        "label_mapping = {label: index for index, label in enumerate(unique_labels)}\n",
        "mapped_labels = train['ManualLabel'].map(label_mapping)\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(mapped_labels),\n",
        "    y=mapped_labels\n",
        ")\n",
        "\n",
        "class_weights_dict = {label_mapping[label]: weight for label, weight in zip(unique_labels, class_weights)}\n",
        "\n",
        "print(class_weights_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXg3odO8c9YV",
        "outputId": "5622a4ef-8cec-4838-9c42-96cb8fd433df"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 1.0550964187327823, 1: 1.563265306122449, 2: 0.7079482439926063}\n"
          ]
        }
      ]
    }
  ]
}